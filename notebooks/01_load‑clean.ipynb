{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T18:34:51.897558Z",
     "start_time": "2025-04-17T18:34:51.569365Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T18:51:37.134440Z",
     "start_time": "2025-04-17T18:51:37.131918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATA_RAW   = Path(\"./data_raw\")\n",
    "DATA_CLEAN = Path(\"./data_clean\")\n",
    "DATA_CLEAN.mkdir(exist_ok=True)"
   ],
   "id": "9daf84c1d837f68c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T18:49:34.156316Z",
     "start_time": "2025-04-17T18:49:34.153276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def sniff_sep(path):\n",
    "    import csv\n",
    "    sample = Path(path).read_text(encoding=\"utf-8\", errors=\"replace\")[:2048]\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[',',';','\\t','|'])\n",
    "        return dialect.delimiter\n",
    "    except:\n",
    "        return ','"
   ],
   "id": "57a59d056e79b641",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load raw CSVs / Excels",
   "id": "d66a408078fb74ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# waiting_times.csv  → raw_waits\n",
    "sep = sniff_sep(DATA_RAW/\"waiting_times.csv\")\n",
    "waits = pd.read_csv(DATA_RAW/\"waiting_times.csv\", sep=sep, encoding=\"utf-8\", low_memory=False)\n",
    "waits.columns = waits.columns.str.strip().str.upper()"
   ],
   "id": "4e6337b98ce35ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# link_attraction_park.csv → link\n",
    "sep = sniff_sep(DATA_RAW / \"link_attraction_park.csv\")\n",
    "link = pd.read_csv(DATA_RAW / \"link_attraction_park.csv\", sep=sep, encoding=\"utf-8\", header=0)\n",
    "link.columns = link.columns.str.strip().str.upper()"
   ],
   "id": "44c2c4c6561b33e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sep = sniff_sep(DATA_RAW/\"attendance.csv\")\n",
    "att = pd.read_csv(DATA_RAW/\"attendance.csv\", sep=sep, encoding=\"utf-8\")\n",
    "# Standardize all column names\n",
    "att.columns = att.columns.str.strip().str.upper()\n",
    "# Then rename the park column\n",
    "att.rename(columns={'FACILITY_NAME': 'PARK'}, inplace=True)\n",
    "\n",
    "# Quick check\n",
    "print(att.columns.tolist())"
   ],
   "id": "227f223b1c310617"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# disneylandreviews.csv → reviews\n",
    "sep = sniff_sep(DATA_RAW / \"disneylandreviews.csv\")\n",
    "reviews = pd.read_csv(DATA_RAW / \"disneylandreviews.csv\", sep=sep, encoding=\"cp1252\")\n",
    "reviews.columns = reviews.columns.str.strip().str.upper()"
   ],
   "id": "186d75bbf72db5ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# weather_data.csv → weather\n",
    "sep = sniff_sep(DATA_RAW / \"weather_data.csv\")\n",
    "weather = pd.read_csv(DATA_RAW / \"weather_data.csv\", sep=sep, encoding=\"utf-8\")\n",
    "weather.columns = weather.columns.str.strip().str.upper()"
   ],
   "id": "d7b86bcba90bb660"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# entity_schedule.csv (optional enrichment)\n",
    "sep = sniff_sep(DATA_RAW / \"entity_schedule.csv\")\n",
    "schedule = pd.read_csv(DATA_RAW / \"entity_schedule.csv\", sep=sep, encoding=\"utf-8\")\n",
    "schedule.columns = schedule.columns.str.strip().str.upper()"
   ],
   "id": "5ed6ba6b98d65990"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# parade_night_show.xlsx (optional)\n",
    "parades = pd.read_excel(DATA_RAW / \"parade_night_show.xlsx\", sheet_name=0)\n",
    "parades.columns = parades.columns.str.strip().str.upper()"
   ],
   "id": "ed809677f2ac7bbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Trim whitespace\n",
    "for df in (waits, link, att, reviews, weather, schedule, parades):\n",
    "    for c in df.columns:\n",
    "        # Only attempt strip on python str types\n",
    "        df[c] = df[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        # Replace empty strings with NA\n",
    "        df[c].replace('', pd.NA, inplace=True)\n",
    "\n",
    "# Quick verification: show any columns where dtype is object but values are not strings\n",
    "for df_name, df in zip(\n",
    "    [\"waits\", \"link\", \"att\", \"reviews\", \"weather\", \"schedule\", \"parades\"],\n",
    "    (waits, link, att, reviews, weather, schedule, parades)\n",
    "):\n",
    "    non_str = df.select_dtypes(include=['object']).map(lambda x: not isinstance(x, str) and pd.notna(x))\n",
    "    count_non_str = non_str.sum().sum()\n",
    "    print(f\"{df_name}: {count_non_str} non-string entries in object columns\")"
   ],
   "id": "f243af7f0c801a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# waiting-times: build datetime & rename\n",
    "waits[\"WORK_DATE\"] = pd.to_datetime(waits[\"WORK_DATE\"])\n",
    "waits[\"HOUR\"]      = waits[\"DEB_TIME_HOUR\"].astype(int)\n",
    "waits[\"DATETIME\"]  = waits[\"WORK_DATE\"] + pd.to_timedelta(waits[\"HOUR\"], unit=\"h\")\n",
    "waits[\"WAIT_MIN\"]  = waits[\"WAIT_TIME_MAX\"].astype(\"Int16\")\n"
   ],
   "id": "7bb650802e3a00d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Build dim_park & dim_ride",
   "id": "dc8ea47c1759a499"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:19:56.924255Z",
     "start_time": "2025-04-17T19:19:56.920222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dim_park\n",
    "dim_park = (\n",
    "    link[[\"PARK\"]]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"PARK_ID\"})\n",
    "    .assign(PARK_ID=lambda d: d[\"PARK_ID\"]+1)\n",
    ")"
   ],
   "id": "5b48163b7cc7f7e6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:20:05.637626Z",
     "start_time": "2025-04-17T19:20:05.631599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dim_ride\n",
    "dim_ride = (\n",
    "    link.merge(dim_park, on=\"PARK\", how=\"left\")\n",
    "        .rename(columns={\"ATTRACTION\":\"RIDE\"})\n",
    "        .drop_duplicates(subset=[\"RIDE\",\"PARK_ID\"])\n",
    "        .reset_index(drop=True)\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\":\"RIDE_ID\"})\n",
    "        .assign(RIDE_ID=lambda d: d[\"RIDE_ID\"]+1)\n",
    ")"
   ],
   "id": "4e4c7f0b4d3ce1af",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:20:22.720481Z",
     "start_time": "2025-04-17T19:20:22.686124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dim_date (calendar)\n",
    "min_d = waits[\"DATETIME\"].dt.floor(\"D\").min()\n",
    "max_d = waits[\"DATETIME\"].dt.floor(\"D\").max()\n",
    "all_dates = pd.DataFrame({\n",
    "    \"DATE\": pd.date_range(min_d, max_d, freq=\"D\")\n",
    "})\n",
    "dim_date = all_dates.assign(\n",
    "    DAY_OF_WEEK = all_dates[\"DATE\"].dt.day_name(),\n",
    "    MONTH_NAME  = all_dates[\"DATE\"].dt.month_name(),\n",
    "    MONTH_NUM   = all_dates[\"DATE\"].dt.month,\n",
    "    YEAR        = all_dates[\"DATE\"].dt.year,\n",
    "    WEEK_NUM    = all_dates[\"DATE\"].dt.isocalendar().week,\n",
    "    IS_WEEKEND  = all_dates[\"DATE\"].dt.dayofweek.isin([5,6])\n",
    ")"
   ],
   "id": "28cddeb3b941e830",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:23:38.997712Z",
     "start_time": "2025-04-17T19:23:38.428057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Add a DATE column to waits for the calendar join\n",
    "waits['DATE'] = waits['DATETIME'].dt.floor('D')\n",
    "\n",
    "# 2. Merge only with dim_ride and dim_date (dim_ride already contains PARK_ID)\n",
    "fact_wait = (\n",
    "    waits\n",
    "    # bring in ride_id and park_id from dim_ride\n",
    "    .merge(\n",
    "        dim_ride[['RIDE_ID', 'RIDE', 'PARK_ID']],\n",
    "        left_on='ENTITY_DESCRIPTION_SHORT',\n",
    "        right_on='RIDE',\n",
    "        how='left'\n",
    "    )\n",
    "    # bring in date dimensions\n",
    "    .merge(\n",
    "        dim_date[['DATE']],\n",
    "        on='DATE',\n",
    "        how='left'\n",
    "    )\n",
    "    # select only the final analytic columns\n",
    "    [['RIDE_ID', 'PARK_ID', 'DATETIME', 'WAIT_MIN', 'CAPACITY', 'GUEST_CARRIED']]\n",
    "    # drop any rows where the join failed\n",
    "    .dropna(subset=['RIDE_ID', 'PARK_ID'])\n",
    ")\n",
    "\n",
    "# Quick sanity check\n",
    "print(f\"fact_wait rows: {len(fact_wait)}\")\n",
    "print(f\"Unique rides mapped: {fact_wait['RIDE_ID'].nunique()}\")"
   ],
   "id": "928a39d4c06437b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_wait rows: 3509324\n",
      "Unique rides mapped: 39\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:41:31.534281Z",
     "start_time": "2025-04-17T19:41:31.532094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# right after you load `att`\n",
    "print(att.columns.tolist())"
   ],
   "id": "1d75dad7a5ac2994",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['USAGE_DATE', 'PARK', 'ATTENDANCE']\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:43:13.084354Z",
     "start_time": "2025-04-17T19:43:13.076756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Standardize the column names\n",
    "att.columns = att.columns.str.strip().str.upper()\n",
    "\n",
    "# 2. Rename USAGE_DATE → DATE\n",
    "att.rename(columns={'USAGE_DATE':'DATE'}, inplace=True)\n",
    "\n",
    "# 3. Build the attendance fact\n",
    "fact_attendance = (\n",
    "    att\n",
    "    .merge(dim_park, on=\"PARK\", how=\"left\")\n",
    "    [['PARK_ID','DATE','ATTENDANCE']]\n",
    "    .dropna(subset=['PARK_ID'])\n",
    ")\n",
    "\n",
    "print(\"fact_attendance rows:\", len(fact_attendance))\n",
    "print(\"Sample:\")\n",
    "print(fact_attendance.head())"
   ],
   "id": "8e01b11cf12a6f6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_attendance rows: 2367\n",
      "Sample:\n",
      "   PARK_ID        DATE  ATTENDANCE\n",
      "0        2  2018-06-01       46804\n",
      "1        1  2018-06-01       20420\n",
      "2        2  2018-06-02       57940\n",
      "3        1  2018-06-02       29110\n",
      "4        2  2018-06-03       44365\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:56:55.237594Z",
     "start_time": "2025-04-17T19:56:55.235288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Show me what columns you really have\n",
    "print(\"reviews columns:\", reviews.columns.tolist())"
   ],
   "id": "c87aa391908e17b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews columns: ['REVIEW_ID', 'RATING', 'YEAR_MONTH', 'REVIEWER_LOCATION', 'REVIEW_TEXT', 'BRANCH', 'SENTIMENT']\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:57:38.307894Z",
     "start_time": "2025-04-17T19:57:38.299576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Rename BRANCH → PARK so it matches dim_park\n",
    "reviews.rename(columns={'BRANCH': 'PARK'}, inplace=True)\n",
    "\n",
    "# 2) Build a full REVIEW_DATE from YEAR_MONTH\n",
    "#    e.g. '2020-05' → Timestamp('2020-05-01')\n",
    "reviews['REVIEW_DATE'] = pd.to_datetime(\n",
    "    reviews['YEAR_MONTH'].astype(str) + \"-01\",\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# 3) Quick check you now have the right columns\n",
    "print(\"reviews cols:\", reviews.columns.tolist())\n",
    "print(reviews[['PARK','YEAR_MONTH','REVIEW_DATE']].head())"
   ],
   "id": "8d5ee2757a286fe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews cols: ['REVIEW_ID', 'RATING', 'YEAR_MONTH', 'REVIEWER_LOCATION', 'REVIEW_TEXT', 'PARK', 'SENTIMENT', 'REVIEW_DATE']\n",
      "                  PARK YEAR_MONTH REVIEW_DATE\n",
      "0  Disneyland_HongKong     2019-4  2019-04-01\n",
      "1  Disneyland_HongKong     2019-5  2019-05-01\n",
      "2  Disneyland_HongKong     2019-4  2019-04-01\n",
      "3  Disneyland_HongKong     2019-4  2019-04-01\n",
      "4  Disneyland_HongKong     2019-4  2019-04-01\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T19:57:59.342929Z",
     "start_time": "2025-04-17T19:57:59.332722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fact_review = (\n",
    "    reviews\n",
    "    .merge(dim_park, on=\"PARK\", how=\"left\")\n",
    "    .rename(columns={'REVIEW_ID': 'ID'})\n",
    "    [['ID', 'PARK_ID', 'REVIEW_DATE', 'RATING', 'SENTIMENT']]\n",
    "    .dropna(subset=['PARK_ID'])\n",
    ")\n",
    "\n",
    "print(\"fact_review rows:\", len(fact_review))\n",
    "fact_review.head()"
   ],
   "id": "669cb91d3fc84432",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact_review rows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, PARK_ID, REVIEW_DATE, RATING, SENTIMENT]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PARK_ID</th>\n",
       "      <th>REVIEW_DATE</th>\n",
       "      <th>RATING</th>\n",
       "      <th>SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:00:31.329056Z",
     "start_time": "2025-04-17T20:00:31.324712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Unique parks in reviews\n",
    "review_parks = set(reviews['PARK'].unique())\n",
    "# 2) Parks in your dim_park\n",
    "dim_parks    = set(dim_park['PARK'])\n",
    "\n",
    "print(\"Review parks:\", review_parks)\n",
    "print(\"Known parks:\", dim_parks)\n",
    "print(\"Missing in dim_park:\", review_parks - dim_parks)"
   ],
   "id": "29e0659e10a93650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review parks: {'Disneyland_HongKong', 'Disneyland_California', 'Disneyland_Paris'}\n",
      "Known parks: {'Tivoli Gardens', 'PortAventura World'}\n",
      "Missing in dim_park: {'Disneyland_HongKong', 'Disneyland_California', 'Disneyland_Paris'}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Persist the five clean tables",
   "id": "4efe5fc3de6f93f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T20:07:09.812603Z",
     "start_time": "2025-04-17T20:07:09.555252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for name, df in [\n",
    "    (\"dim_park\", dim_park),\n",
    "    (\"dim_ride\", dim_ride),\n",
    "    (\"dim_date\", dim_date),\n",
    "    (\"fact_wait\", fact_wait),\n",
    "    (\"fact_attendance\", fact_attendance),\n",
    "    (\"fact_review\", fact_review)\n",
    "]:\n",
    "    df.to_parquet(DATA_CLEAN/f\"{name}.parquet\", index=False)\n",
    "print(\"✅  All clean tables written to data_clean/ 🎉\")"
   ],
   "id": "f52524209b75c0ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  All clean tables written to data_clean/ 🎉\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54340375d469d333"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5f93d69ea74940dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "72f91e385cac5cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f3b4113a21bc5f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8f02fb29e4cd11f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2bbfc2870236f1c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be7124cc34087ca0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "140ef5a6819007fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80d13e8524bfd788"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8ace0f579aec8394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c6bf7746fb1ae37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
